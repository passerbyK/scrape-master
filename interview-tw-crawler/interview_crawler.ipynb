{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d29cccc",
   "metadata": {},
   "source": [
    "# Interview TW Crawler v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eeb2ed",
   "metadata": {},
   "source": [
    "Updated on 2024/08/07, by Xiang-Yi Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe0290",
   "metadata": {},
   "source": [
    "### Step 0: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install fake_useragent\n",
    "!pip install webdriver_manager\n",
    "!pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823df453",
   "metadata": {},
   "source": [
    "### Step 1: Input Your Google Account\n",
    "You need to have a VIP account for this website.\n",
    "\n",
    "In this version, please notice that you need to artificially verify your Gmail account before crawling the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3493862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs: Try to avoid verification.\n",
    "gmail = 'xxxx@gmail.com'\n",
    "password = 'xxxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625719fc",
   "metadata": {},
   "source": [
    "### Step 2: Let's Crawl !\n",
    "In this version, please enter all the company URLs you want to crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedafa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs: Get all the company URLs on the website.\n",
    "company_url_list = [\"A1St\", \"0noW8\", \"ECKNd\"] # This is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs: Use fake useragent to avoid detection, and other crawling techniques.\n",
    "# TODOs: Try to avoid robot detection. There are 2 conditions after you are detected: 1. Simply ask if you are a robot. 2. You are asked to solve a CAPTCHA.\n",
    "# TODOs: Customize the crawler that you can crawl the data you want.\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Other resources: https://github.com/seleniumbase/SeleniumBase\n",
    "# Initialize undetected-chromedriver\n",
    "driver = uc.Chrome()\n",
    "        \n",
    "def google_login(gmail, password):\n",
    "    # Wait for and click the \"Continue with Google\" button\n",
    "    try:\n",
    "        google_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//span[text()='使用 Google 繼續']\"))\n",
    "        )\n",
    "        google_button.click()\n",
    "    except:\n",
    "        google_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//span[text()='Continue with Google']\"))\n",
    "        )\n",
    "        google_button.click()\n",
    "        \n",
    "    # Wait for Google login page to load and enter email\n",
    "    email_input = WebDriverWait(driver, 5).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//input[@type='email']\"))\n",
    "    )\n",
    "    email_input.send_keys(gmail)\n",
    "    email_input.send_keys('\\n')  # Press enter\n",
    "\n",
    "    # Wait for password page to load and enter password\n",
    "    password_input = WebDriverWait(driver, 5).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, \"//input[@type='password']\"))\n",
    "    )\n",
    "    password_input.send_keys(password)\n",
    "    password_input.send_keys('\\n')  # Press enter\n",
    "\n",
    "    # You have 60 seconds to artificially verify your Google account.\n",
    "    time.sleep(60)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    if \"interview.tw\" in current_url:\n",
    "        print(\"Successfully logged in and redirected to interview.tw page\")\n",
    "    else:\n",
    "        print(\"Failed to log in or redirect to the expected page\")\n",
    "        print(\"Stop crawling\")\n",
    "        driver.quit()\n",
    "        \n",
    "def interview_crawler(company_url, page_number, data):\n",
    "    # Get the company name\n",
    "    try:\n",
    "        driver.get(f\"https://interview.tw/c/{company_url}\")\n",
    "        company_name = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h1[@class='fz-headline-sm fz-tit']\"))\n",
    "        ).text\n",
    "        print(f\"Company name now: {company_name}\")\n",
    "    except Exception as e:\n",
    "        company_name = \"NONE\"\n",
    "        print(f\"Error retrieving company name: {e}\")\n",
    "        \n",
    "    while True:\n",
    "        # Enter company page\n",
    "        try:\n",
    "            driver.get(f\"https://interview.tw/c/{company_url}?page={page_number}&sort=newest\")\n",
    "            \n",
    "            # Check for 404 error or no filter results\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//span[text()='404']\"))\n",
    "                )\n",
    "                print(f\"Page returned 404 error, ending crawler for {company_name}\")\n",
    "                break\n",
    "            except:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, \"//div[text()='沒有篩選結果']\"))\n",
    "                    )\n",
    "                    print(f\"Page shows no filter results, ending crawler for {company_name}\")\n",
    "                    break\n",
    "                except:\n",
    "                    pass  # Continue execution if these elements are not found\n",
    "        except:\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"No page {page_number}, ending crawler for {company_name}\")\n",
    "            break\n",
    "        \n",
    "        # Look for VIP button and click\n",
    "        try:\n",
    "            vip_buttons = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//span[text()='使用 VIP 解鎖']\"))\n",
    "            )\n",
    "            for i, button in enumerate(vip_buttons):\n",
    "                button.click()\n",
    "                time.sleep(10)\n",
    "\n",
    "                # Usually happens after clicking 3 VIP buttons\n",
    "                if i == 2:\n",
    "                    try:\n",
    "                        reconsider_button = WebDriverWait(driver, 5).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"//span[text()='再考慮']\"))\n",
    "                        )\n",
    "                        reconsider_button.click()\n",
    "                        time.sleep(10)\n",
    "                    except:\n",
    "                        print(\"No '再考慮' button\")\n",
    "                        pass\n",
    "            total_vip_buttons = len(vip_buttons)\n",
    "                    \n",
    "            # If VIP button not found, check for related articles and crawl directly\n",
    "            try:\n",
    "                experience_items = driver.find_elements(By.XPATH, \"//div[@class='iw_experience-item--body']\")\n",
    "                for item in experience_items[: total_vip_buttons]:\n",
    "                    # Initialize values in the table\n",
    "                    position = \"\"\n",
    "                    location = \"\"\n",
    "                    share_date = \"\"\n",
    "                    interview_date = \"\"\n",
    "                    interview_rating = \"\"\n",
    "                    interview_status = \"\"\n",
    "                    interview_difficulty = \"\"\n",
    "                    interview_process = \"\"\n",
    "                    interview_qa = \"\"\n",
    "                    interview_advice = \"\"\n",
    "                    tags = \"\"\n",
    "                    \n",
    "                    # Exception handling for various cases\n",
    "                    try:\n",
    "                        spans_in_item = item.find_elements(By.TAG_NAME, \"span\")\n",
    "                        span_texts = [span.text for span in spans_in_item]\n",
    "\n",
    "                        # Ensure text list has enough elements\n",
    "                        if len(span_texts) >= 12:\n",
    "                            interview_date_index = span_texts.index(\"面試時間\")\n",
    "                            \n",
    "                            position = span_texts[0]\n",
    "                            location = span_texts[1]\n",
    "                            share_date = span_texts[3].replace(\" 分享\", \"\").strip()\n",
    "                            interview_date = span_texts[interview_date_index - 1]\n",
    "                            interview_rating = span_texts[interview_date_index + 1]\n",
    "                            interview_status = span_texts[interview_date_index + 4]\n",
    "                            interview_difficulty = span_texts[interview_date_index + 6]\n",
    "\n",
    "                            # Ensure specific fields exist\n",
    "                            if \"面試過程\" in span_texts:\n",
    "                                interview_process = span_texts[span_texts.index(\"面試過程\") + 1]\n",
    "                            if \"面試問答\" in span_texts and \"面試建議\" in span_texts:\n",
    "                                interview_qa = \"\\n\".join(span_texts[span_texts.index(\"面試問答\") + 1: span_texts.index(\"面試建議\")])\n",
    "                            if \"面試建議\" in span_texts and interview_status in span_texts:\n",
    "                                interview_advice = \"\\n\".join(span_texts[span_texts.index(\"面試建議\") + 1: span_texts.index(interview_status, 12)])\n",
    "                            if interview_status in span_texts:\n",
    "                                tags = \",\".join(span_texts[span_texts.index(interview_status, 12): span_texts.index(\"分享\") - 2])\n",
    "                        else:\n",
    "                            print(\"Warning: span_texts length is less than expected.\")\n",
    "                        \n",
    "                    except IndexError as e:\n",
    "                        print(f\"IndexError occurred: {e}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"ValueError occurred: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An unexpected error occurred: {e}\")\n",
    "                        print(\"Unable to crawl data!\")\n",
    "\n",
    "                    # Add extracted data to the list\n",
    "                    data.append({\n",
    "                        \"公司名稱\": company_name,\n",
    "                        \"職位\": position,\n",
    "                        \"工作地點\": location,\n",
    "                        \"分享時間\": share_date,\n",
    "                        \"面試時間\": interview_date,\n",
    "                        \"面試評價\": interview_rating,\n",
    "                        \"面試狀態\": interview_status,\n",
    "                        \"面試難度\": interview_difficulty,\n",
    "                        \"面試過程\": interview_process,\n",
    "                        \"面試問答\": interview_qa,\n",
    "                        \"面試建議\": interview_advice,\n",
    "                        \"標籤\": tags\n",
    "                    })\n",
    "                \n",
    "                page_number += 1\n",
    "                print(f\"Moving to page {page_number}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                print(\"Unable to crawl data!\")\n",
    "        except:           \n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"No more VIP buttons or VIP articles to fetch, ending crawler for {company_name}\")\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# Start crawling\n",
    "try:\n",
    "    # Open the specified login page\n",
    "    URL = \"https://interview.tw/auth/login?redirect=https%3A%2F%2Finterview.tw\"\n",
    "    driver.get(URL)\n",
    "\n",
    "    # Login with Google account\n",
    "    google_login(gmail, password)\n",
    "    \n",
    "    # Enter company URLs, please note that if you have already unlocked VIP articles for a company, there might be an error. Please try a new company first.\n",
    "    columns = [\"公司名稱\", \"職位\", \"工作地點\", \"分享時間\", \"面試時間\", \"面試評價\", \"面試狀態\", \"面試難度\", \"面試過程\", \"面試問答\", \"面試建議\", \"標籤\"]\n",
    "    df_all = pd.DataFrame(columns = columns)\n",
    "    for company_url in company_url_list:\n",
    "        df = interview_crawler(company_url = company_url, page_number = 1, data = [])\n",
    "        df_all = pd.concat([df_all, df], ignore_index = True)\n",
    "finally:\n",
    "    # Close the browser when done\n",
    "    driver.quit()\n",
    "\n",
    "df_all.to_csv(\"Interview_Data.csv\", index = False, encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04def12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
