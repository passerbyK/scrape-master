{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb70ecd",
   "metadata": {},
   "source": [
    "# Data Mining: Tripadvisor\n",
    "by Xiang-Yi, Huang (xiangyi.huang0213@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e139df8",
   "metadata": {},
   "source": [
    "## 1. Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Packages:\n",
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install fake_useragent\n",
    "\n",
    "# More Packages: re, datetime, time, json, warnings\n",
    "# You may use these packages if you crawl one by one：os、glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217e309",
   "metadata": {},
   "source": [
    "## 2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da064b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "start = datetime.now() # calculate execution time\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "headers = {'user-agent': user_agent}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver', options = options)\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# TripAdvisor TOP 10 Popular Taipei Hotels in 2022\n",
    "url = 'https://www.tripadvisor.com.tw/Hotels-g293913-Taipei-Hotels.html'\n",
    "driver.get(url)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "title = soup.find_all('a', {'class': 'property_title prominent'}) # Find hotel names\n",
    "\n",
    "jsonlist = []\n",
    "current_year = datetime.now().year # Get the current year for comment timestamps\n",
    "\n",
    "hotel = 1 # hotel = 0 might be an ad-recommended hotel, and it may overlap with the next hotel, so skip it. Modify the number here if fetching one at a time.\n",
    "hotel_num = 0 # Number of hotels with reviews fetched, modify this to fetch a different number of hotels\n",
    "\n",
    "while True:\n",
    "    hotel_name = title[hotel].text.strip() \n",
    "    try: # Sometimes, the hotel information may not be captured. If not, move on to the next hotel.\n",
    "        if hotel >= 7: # When capturing the eighth hotel or later, there might be a \"View All\" button, which needs to be clicked.\n",
    "            try:\n",
    "                find_all = driver.find_element(By.XPATH, '//*[@id=\"component_6\"]/div/button')\n",
    "                find_all.click()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        hotel_page = driver.find_element(By.LINK_TEXT, hotel_name)\n",
    "        hotel_page.click() # Click on the hotel name to go to the hotel's details page\n",
    "        print('start to crawl: ', hotel_page.text)\n",
    "        time.sleep(2)\n",
    "    \n",
    "        driver.switch_to.window(driver.window_handles[1]) # After clicking on the hotel's details page, a new tab is opened, so switch to that tab\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "        # Capture various data, but TripAdvisor does not have the hotel's phone number available, so it is omitted. However, additional features of the hotel are captured.\n",
    "        name = soup.find('h1', {'class': 'QdLfr b d Pn'}).text # Hotel name\n",
    "        grade = soup.find('span', {'class': 'uwJeR P'}).text # Overall rating, out of five\n",
    "        star = soup.find('svg', {'class': 'JXZuC d H0'}).get('aria-label')[0:4] # Star rating, up to five stars\n",
    "        introduction = soup.find('div', {'class': 'fIrGe _T'}).text # Hotel introduction\n",
    "        address = soup.find('span', {'class': 'fHvkI PTrfg'}).text # Address\n",
    "        \n",
    "        feature = soup.find_all('div', {'class': 'yplav f ME H3 _c'}) # All features\n",
    "        feature_list = []\n",
    "        for num in range(len(feature)):\n",
    "            feature_list.append(feature[num].text)\n",
    "    \n",
    "        comment_jsonlist = []\n",
    "        id_num = 1 # The nth comment\n",
    "        \n",
    "        print('start to crawl all comments!')\n",
    "        while True:\n",
    "            \n",
    "            # Get various comment data. Image locations are not found in the original HTML file, so they are omitted.\n",
    "            comment = soup.find_all('div', {'class': 'YibKl MC R2 Gi z Z BB pBbQr'}) # First, capture all comments on the page\n",
    "            \n",
    "            # The data is complex, so each field is handled separately to prevent potential issues\n",
    "            for k in range(len(comment)): # Number of comments on the page\n",
    "                \n",
    "                # Usernames may disappear, appearing as \"Tripadvisor Member\"\n",
    "                try:\n",
    "                    cmt_name = comment[k].find('a', {'class': 'ui_header_link uyyBf'}).text\n",
    "                except:\n",
    "                    cmt_name = 'Tripadvisor Member'\n",
    "            \n",
    "                # Star ratings may not always be available\n",
    "                try:\n",
    "                    cmt_star = str(int(comment[k].find('div', {'class': 'Hlmiy F1'}).span.get('class')[1][-2:]) / 10.0)\n",
    "                except:\n",
    "                    cmt_star = 'None'\n",
    "        \n",
    "                # Handle comment timestamp\n",
    "                try:\n",
    "                    cmt_time = comment[k].find('div', {'class': 'cRVSd'}).text\n",
    "                    cmt_time = re.search('(?<=Posted a review on ).+', cmt_time).group(0)\n",
    "                    if cmt_time.find('year') == -1:\n",
    "                        cmt_time = str(current_year) + ' ' + cmt_time\n",
    "                except:\n",
    "                    cmt_time = 'None'\n",
    "                \n",
    "                # Stay time may not always be available\n",
    "                try:\n",
    "                    sty_time = comment[k].find('span', {'class': 'teHYY _R Me S4 H3'}).text\n",
    "                    sty_time = re.search('(?<=Date of stay: ).+', sty_time).group(0)\n",
    "                except:\n",
    "                    sty_time = 'None'\n",
    "                \n",
    "                # Handle comment title\n",
    "                try:\n",
    "                    cmt_content_title = comment[k].find('div', {'class': 'KgQgP MC _S b S6 H5 _a'}).text\n",
    "                except:\n",
    "                    cmt_content_title = 'None'\n",
    "            \n",
    "                # Handle full comment text\n",
    "                try:\n",
    "                    cmt_content = comment[k].find('q', {'class': 'QewHA H4 _a'}).text\n",
    "                except:\n",
    "                    cmt_content = 'None'\n",
    "        \n",
    "                # Likes on comments may not always be available\n",
    "                try:\n",
    "                    cmt_likes_num = comment[k].find_all('span', {'class': 'yRNgz'})[1].text\n",
    "                except:\n",
    "                    cmt_likes_num = '0'\n",
    "        \n",
    "                # Store the captured data in JSON format:\n",
    "                cmt_dict_temp = {}\n",
    "                cmt_dict_temp['id'] = id_num # The nth comment\n",
    "                cmt_dict_temp['comment_name'] = cmt_name # Comment username\n",
    "                cmt_dict_temp['comment_star'] = cmt_star # Comment star rating  \n",
    "                cmt_dict_temp['comment_time'] = cmt_time # Comment timestamp\n",
    "                cmt_dict_temp['stay_time'] = sty_time # Stay time\n",
    "                cmt_dict_temp['comment_content_title'] =  cmt_content_title # Comment title\n",
    "                cmt_dict_temp['comment_content'] = cmt_content # Full comment text\n",
    "                cmt_dict_temp['comment_likes_num'] = cmt_likes_num # Comment likes\n",
    "                comment_jsonlist.append(cmt_dict_temp)\n",
    "        \n",
    "                id_num += 1\n",
    "                \n",
    "                # Check where the current capture is at\n",
    "                if id_num % 100 == 0:\n",
    "                    print('here is id: ', id_num)\n",
    "        \n",
    "            try:\n",
    "                # Not sure if it might get locked, but still switch user_agent just in case\n",
    "                ua = UserAgent()\n",
    "                user_agent = ua.random\n",
    "                headers = {'user-agent': user_agent}\n",
    "                \n",
    "                comment_page = driver.find_element(By.LINK_TEXT, 'Next').click() # Go to the next page\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')  \n",
    "                \n",
    "            except: # If it's the last page, break out\n",
    "                # Store the captured data in JSON format:\n",
    "                dict_temp = {}\n",
    "                dict_temp['id'] = hotel_num + 1 # If fetching one at a time, this id needs to be modified\n",
    "                dict_temp['name'] = name\n",
    "                dict_temp['grade'] = grade\n",
    "                dict_temp['star'] = star\n",
    "                dict_temp['introduction'] = introduction\n",
    "                dict_temp['address'] = address\n",
    "                dict_temp['feature'] = feature_list\n",
    "                dict_temp['comment'] = comment_jsonlist\n",
    "                jsonlist.append(dict_temp)\n",
    "                break    \n",
    "\n",
    "    \n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # Switch back to the original page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        hotel += 1\n",
    "        hotel_num += 1 \n",
    "        print(name, ' is done!')  # Indicates successful data retrieval for a hotel\n",
    "    \n",
    "    except:  # If unable to fetch data for the current hotel, move on to the next one\n",
    "        hotel += 1\n",
    "    \n",
    "    if hotel == 20:  # At most, there might be one or two hotels that couldn't be fetched; too many indicates another issue\n",
    "        break\n",
    "    \n",
    "    if hotel_num == 10:  # Indicates successful data retrieval for ten hotels! Mission accomplished! # If fetching one at a time, it would be hotel_num == 1\n",
    "        print('Everything is done!')\n",
    "        break\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "# Save all data and generate a JSON file\n",
    "with open('crawler_homework.json', 'w', encoding = 'utf-8') as file:  # If fetching one at a time, the filename needs to be changed\n",
    "    file.write(json.dumps(json_list, indent = 2, ensure_ascii = False))\n",
    "    \n",
    "end = datetime.now()\n",
    "print('Total execution time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b151792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is done!\n"
     ]
    }
   ],
   "source": [
    "# When fetching one hotel at a time, name each JSON file as crawler_homework_X.json, where X = 0 ~ 9\n",
    "# When fetching one hotel at a time, merge all JSON files\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in glob.glob(os.path.join('./', '*.json')): # Fetch each JSON file\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        output = json.load(f)\n",
    "        data.extend(output) # Merge each JSON file\n",
    "        \n",
    "with open('crawler_homework.json', 'w', encoding='utf-8') as file: # Convert the merged data back to a JSON file\n",
    "    file.write(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "    \n",
    "print('Everything is done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49e408",
   "metadata": {},
   "source": [
    "## 3. Brief Discussion\n",
    "1. I haven't fully grasped the logic of TripAdvisor, so when trying to fetch ten hotels at once, it sometimes fails -> I speculate it might be influenced by advertisements.\n",
    "2. There are many inconsistent formats within TripAdvisor, so I had to rely on exception handling to address the issue.\n",
    "3. Fetching ten hotels at once takes too long, so it's better to fetch them in batches, and each batch is more likely to succeed. (The test run above was interrupted, and it can be observed that the \"Mandarin Oriental\" hotel was captured twice.)\n",
    "4. The above code is designed to fetch ten hotels at once. If you want to change it to fetching one at a time, you can make the following modifications:\n",
    "    * 4-1. Variable \"hotel\" needs to be changed, and you can set it from 1 to 20 without any issues.\n",
    "    * 4-2. \"dicttemp['id']\" also needs to be changed, such as setting \"dicttemp['id'] = 10\".\n",
    "    * 4-3. Change \"if hotel_num == 10:\" to \"if hotel_num == 1:\".\n",
    "    * 4-4. The file name needs to be changed, for example, 'crawler_homework_1.json'.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
